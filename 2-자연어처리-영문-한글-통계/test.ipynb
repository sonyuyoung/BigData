{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영문 분석 -> 워드 클라우드로 그리기(시각화)\n",
    "샘플 데이터 영문 학술 문서의 제목만 추출, 그 단어의 빈도 분석 시각화\n",
    "데이터 수집 : Big data 키워드로 검색 후, 해당 학술 연구 정보 서비스에서 수집 해보기.\n",
    "조합, pandas.concat(), 정제 re 정규식, 기본적인 유효성 체크.\n",
    "변환 : word_tokenize(), lower(),\n",
    "matplotlib.pyplot 이용하기.\n",
    "단어 빈도 구해주는 Counter() 이용.\n",
    "비정형 빅데이터 분석을 말하고 -> 자연어 처리 (nature language processing )\n",
    "자연어 처리 예) 음성, 텍스트 정보 추출.\n",
    "단어 빈도를 추출해서, 해당 단어 시각화하기.\n",
    "\n",
    "\n",
    "[관련 단어 용어 정리]\n",
    "텍스트 분석: 자연어 처리와 데이터 마이닝 결합하여 발전되었고,\n",
    "비정형 텍스트 데이터에서 정보를 추출하는 분석 방법.\n",
    "분석 방법 : 1) 텍스트 분류 2) 텍스트 군집화 3) 감성 분석.\n",
    "전처리 : 분석 작업의 정확도를 높이기 위해서 사용할 데이터 정리하고 변환하는 작업.\n",
    "수행하는 작업\n",
    "정제 (cleaning): 불필요한 기호, 문자 필터하는 작업, 정규식을 이용해서 작업을 함.\n",
    "정규화 (normallization) : 형태가 다른 단어를 특정의 형태로 변환 작업 , 대문자, 소문자 통합 하는 작업, 의미가 비슷한 단어끼리 통합작업.\n",
    "토큰화 (tokenization) : 토큰으로 정하는 기본 단위로 분리 작업. 문장 기준, 단어 기준이 될수 도 있다.\n",
    "불용어제거(stopword) : 의미 있는 단어를 추출하기 위해서, 조사, 관사, 접미사, 접두사 등. 제거하는 작업.\n",
    "어간 추출(semming) : 단수, 복수, 진행형(시제), 분리하는 작업\n",
    "표제어 추출(lemmatization ): 단어의 기본형 형태로 일반화 하는 작업.\n",
    "예)\n",
    "Gone -> go\n",
    "am -> be\n",
    "going -> go\n",
    "워드클라우드 : 텍스트 분석에서 빈도를 시각화 할 때 많이 사용됨.\n",
    "데이터 수집\n",
    "한국교육학술정보원 (KERIS)의 RISS 사이트\n",
    "https://www.riss.kr/index.do\n",
    "Big data 검색해보기.\n",
    "한 페이지당 100개씩 내보내기 엑셀 파일 간략 정보 , 반복 10번\n",
    "1000개의 데이터에서 제목만 추출 및 분류 작업하기.\n",
    "데이터 준비 작업.\n",
    "제목 컬럼 빈도 분석 해보기.\n",
    "\n",
    "nltk=Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 관련 패키지들 임포트 하기. \n",
    "import pandas as pd \n",
    "# 경로 이름 지정해서 파일 처리할 때 사용하는 도구\n",
    "import glob \n",
    "# 정규 표현식에 사용하는 도구 \n",
    "import re \n",
    "# 2차원 리스트를 -> 1차원 리스트로 차원 축소시 사용하는 도구 \n",
    "from functools import reduce\n",
    "# 자연어 처리 패키지 중에서, 단어 토큰화 작업.\n",
    "from nltk.tokenize import word_tokenize\n",
    "# 불용어 처리 작업. \n",
    "from nltk.corpus import stopwords \n",
    "# 표제어 추출 \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# 단어의 빈도를 추출하는 도구. \n",
    "from collections import Counter \n",
    "import matplotlib.pyplot as plt\n",
    "# 단어의 빈도수를 시각화하는 도구, 빈도가 높을수록 글자 크기가 커짐. \n",
    "from wordcloud import STOPWORDS, WordCloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>번호</th>\n",
       "      <th>제목</th>\n",
       "      <th>저자</th>\n",
       "      <th>출판사</th>\n",
       "      <th>출판일</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Big-Data Challenge Advances highlight big issues</td>\n",
       "      <td>unknown</td>\n",
       "      <td>BEIJING REVIEW</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Big-Data-Driven Stem Cell Science and Tissue E...</td>\n",
       "      <td>Del Sol, Antonio; Thiesen, Hans J.; Imitola, J...</td>\n",
       "      <td>ELSEVIER SCIENCE B.V; AMSTERDAM</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Big-data driven functional interaction pattern...</td>\n",
       "      <td>Zheng, Minrui</td>\n",
       "      <td>SCIENCE PRESS</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>Big-Data and Simulations of Social and Economi...</td>\n",
       "      <td>Takayasu, H.</td>\n",
       "      <td>JAPAN TECHNICAL INFORMATION SERVICE</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>Big-Data Science: Infrastructure Impact</td>\n",
       "      <td>Monga, Inder; Prabhat,</td>\n",
       "      <td>INDIAN NATIONAL SCIENCE ACADEMY</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>Careers: Graduate programs for big-data analysts</td>\n",
       "      <td>unknown</td>\n",
       "      <td>IEEE INSTITUTE OF ELECTRICAL AND ELECTRONICS</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NaN</td>\n",
       "      <td>97</td>\n",
       "      <td>PMU121 APPLICATION OF REAL-WORLD DATA FROM MED...</td>\n",
       "      <td>Wang, Y.; Xing, Y.; Wu, Y.; Yuan, N.; Wang, F....</td>\n",
       "      <td>Elsevier Science B.V., Amsterdam</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>98</td>\n",
       "      <td>Design and Implementation of Big-Data Analysis...</td>\n",
       "      <td>Zhang, Pan; Ding, Lingyun; Jiang, Ning; Ling, ...</td>\n",
       "      <td>Elsevier Science B.V., Amsterdam</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>Performance Meta-analysis for Big-Data Univari...</td>\n",
       "      <td>Stefanopoulou, Aliki</td>\n",
       "      <td>Springer</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>After Facebook Fiasco, Big-Data Researchers Re...</td>\n",
       "      <td>VOOSEN, PAUL.</td>\n",
       "      <td>Chronicle of higher education, etc</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0   번호                                                 제목  \\\n",
       "0          NaN    1   Big-Data Challenge Advances highlight big issues   \n",
       "1          NaN    2  Big-Data-Driven Stem Cell Science and Tissue E...   \n",
       "2          NaN    3  Big-data driven functional interaction pattern...   \n",
       "3          NaN    4  Big-Data and Simulations of Social and Economi...   \n",
       "4          NaN    5            Big-Data Science: Infrastructure Impact   \n",
       "..         ...  ...                                                ...   \n",
       "95         NaN   96   Careers: Graduate programs for big-data analysts   \n",
       "96         NaN   97  PMU121 APPLICATION OF REAL-WORLD DATA FROM MED...   \n",
       "97         NaN   98  Design and Implementation of Big-Data Analysis...   \n",
       "98         NaN   99  Performance Meta-analysis for Big-Data Univari...   \n",
       "99         NaN  100  After Facebook Fiasco, Big-Data Researchers Re...   \n",
       "\n",
       "                                                   저자  \\\n",
       "0                                             unknown   \n",
       "1   Del Sol, Antonio; Thiesen, Hans J.; Imitola, J...   \n",
       "2                                       Zheng, Minrui   \n",
       "3                                        Takayasu, H.   \n",
       "4                              Monga, Inder; Prabhat,   \n",
       "..                                                ...   \n",
       "95                                            unknown   \n",
       "96  Wang, Y.; Xing, Y.; Wu, Y.; Yuan, N.; Wang, F....   \n",
       "97  Zhang, Pan; Ding, Lingyun; Jiang, Ning; Ling, ...   \n",
       "98                               Stefanopoulou, Aliki   \n",
       "99                                      VOOSEN, PAUL.   \n",
       "\n",
       "                                             출판사   출판일  \n",
       "0                                 BEIJING REVIEW  2014  \n",
       "1                ELSEVIER SCIENCE B.V; AMSTERDAM  2017  \n",
       "2                                  SCIENCE PRESS  2022  \n",
       "3            JAPAN TECHNICAL INFORMATION SERVICE  2014  \n",
       "4                INDIAN NATIONAL SCIENCE ACADEMY  2018  \n",
       "..                                           ...   ...  \n",
       "95  IEEE INSTITUTE OF ELECTRICAL AND ELECTRONICS  2013  \n",
       "96              Elsevier Science B.V., Amsterdam  2020  \n",
       "97              Elsevier Science B.V., Amsterdam  2018  \n",
       "98                                      Springer  2022  \n",
       "99            Chronicle of higher education, etc  2015  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 조합(병합)하기. \n",
    "# 현재 폴더 내부에 있는 , 받았던 엑셀 파일명 10개를 선택하기. \n",
    "all_files = glob.glob(\"./myCabinetExcelData*.xls\")\n",
    "all_files\n",
    "\n",
    "# 엑셀 파일 읽어서 -> 데이터 프레임 (표형태) 변환 ->특정 리스트에 담아두기 \n",
    "# 임시로 저장할 리스트 변수 \n",
    "all_files_data = []\n",
    "\n",
    "# all_files 에 담겨진 엑셀 파일의 위치가 들어있고, \n",
    "# 해당 위치의 엑셀 파일을 읽어서, 데이터 프레임 표 형태로 변환하기. \n",
    "# 임시 리스트에 담기. \n",
    "# 반복문\n",
    "for file in all_files:\n",
    "  # 해당 엑셀 파일의 위치의 물리 파일 읽기 pd.read_excel(file)\n",
    "  data_frame = pd.read_excel(file)\n",
    "  # 임시 리스트에 담기. \n",
    "  all_files_data.append(data_frame)\n",
    "\n",
    "# 샘플 확인 해보기., 첫번째 요소 확인 해보기. \n",
    "# all_files_data = [엑셀1,엑셀2,엑셀3,...엑셀10]\n",
    "all_files_data[0]\n",
    "\n",
    "# 오류 발생, 모듈 미설 : xlrd\n",
    "# cmd -> pip install xlrd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/u020/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/u020/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# # Download NLTK stopwords data\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_files_data_concat.shape: (1000, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 엑셀 파일 10개를 병합해서 출력해보기. \n",
    "# axis=0 , 세로 방향으로 , 밑으로 데이터를 붙이는 작업. \n",
    "all_files_data_concat = pd.concat(all_files_data, axis=0, ignore_index=True)\n",
    "all_files_data_concat.shape\n",
    "print(f\"all_files_data_concat.shape: {all_files_data_concat.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# 병합된 파일을 csv 파일로 변환하기. \n",
    "all_files_data_concat.to_csv(\"./riss_Bigdata.csv\", encoding=\"utf-8\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 작업하기\n",
    "all_title = all_files_data_concat[\"제목\"]\n",
    "all_title\n",
    "\n",
    "# 불용어 제거\n",
    "# 불용어제거(stopword) : 의미 있는 단어를 추출하기 위해서, 조사, 관사, 접미사, 접두사 등. 제거하는 작업. \n",
    "# set(),기본적으로 중복 제거 \n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "#lemma 작업 표제어 추출 \n",
    "#표제어 추출(lemmatization ): 단어의 기본형 형태로 일반화 하는 작업. 시제 제거 \n",
    "\n",
    "lemma = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m enWordsToken \u001b[39m=\u001b[39m word_tokenize(enWords\u001b[39m.\u001b[39mlower())\n\u001b[1;32m     10\u001b[0m \u001b[39m# 컴프리헨션 파이썬 스타일 코드로 , 리스로 변경하기. \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[39m# 불용어 제거 작업, 조사, 관사 등 제거하는 작업. -> 오타:stopwords -> stopWords\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m enWordsTokenStop\u001b[39m=\u001b[39m [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m enWordsToken \u001b[39mif\u001b[39;00m w \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopWords]\n\u001b[1;32m     15\u001b[0m \u001b[39m# 표제어 추출 작업. \u001b[39;00m\n\u001b[1;32m     16\u001b[0m enWordsTokenStopLemma \u001b[39m=\u001b[39m [lemma\u001b[39m.\u001b[39mlemmatize(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m enWordsTokenStop]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopWords' is not defined"
     ]
    }
   ],
   "source": [
    "# 임시로 담을 변수, \n",
    "words = []\n",
    "\n",
    "# 기본적인 , 단어 추출(전처리 작업.)\n",
    "for title in all_title:\n",
    "  # 영문자, 대 소문자를 제외한 나머지 문자는 모두 공백으로 처리하겠다. \n",
    "  enWords = re.sub(r\"[^a-zA-Z]+\",\" \",str(title))\n",
    "  # 남은 영문자를 -> 모두 소문자 변환하고 -> 각단어를 토큰으로 각각 분리 작업. \n",
    "  enWordsToken = word_tokenize(enWords.lower())\n",
    "  # 컴프리헨션 파이썬 스타일 코드로 , 리스로 변경하기. \n",
    "  \n",
    "  # 불용어 제거 작업, 조사, 관사 등 제거하는 작업. -> 오타:stopwords -> stopWords\n",
    "  enWordsTokenStop= [w for w in enWordsToken if w not in stopwords]\n",
    "\n",
    "  # 표제어 추출 작업. \n",
    "  enWordsTokenStopLemma = [lemma.lemmatize(w) for w in enWordsTokenStop]\n",
    "\n",
    "  words.append(enWordsTokenStopLemma)\n",
    "  \n",
    "print(f\"words(전처리, re 정규식, token화, stop 불용어 제거, 표제어 추출) : {words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
